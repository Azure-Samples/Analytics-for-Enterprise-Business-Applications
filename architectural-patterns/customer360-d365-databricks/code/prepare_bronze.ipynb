{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45798058-6eeb-4dc2-b398-e893fe571a49",
     "showTitle": true,
     "title": "Prepare BRONZE"
    }
   },
   "source": [
    "The export done by the *Synaps Link for Dataverse* is stored in the *dataverse-ENVIRONMENT-GUID* container. This notebook copies this CSV export to the **BRONZE** layer container.\n",
    "\n",
    "1. Read the model json file from the *dataverse-ENVIRONMENT-GUID* => *Microsoft.Athena.TrickleFeedService* directory and extract the column names. \n",
    "2. Read the csv export data under the table name directory, add the column names to the dataframe.\n",
    "3. Save the dataframe to the Bronze layer. Use date as partition folder name.\n",
    "2. Manually upload the external resident data to the residents_external directiry in the raw container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff4b62dd-c3e9-4a97-bc3c-7b041d635d76",
     "showTitle": true,
     "title": "Spark Config"
    }
   },
   "outputs": [],
   "source": [
    "# configs\n",
    "storage_account_name = \"[storage_account_name]\"\n",
    "tenant_id = \"[tenant_id]\"\n",
    "client_id = \"[client_id]\"\n",
    "client_secret = \"[client_secret]\"\n",
    "\n",
    "# spark configuration options\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\",\"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce2f6c1d-5409-46d4-b11a-2adb6b7e7033",
     "showTitle": true,
     "title": "Dataframe from dataverse table with header"
    }
   },
   "outputs": [],
   "source": [
    "# Dataframe from dataverse table with header\n",
    "\n",
    "import json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "dataverse_link_container_name = \"[dataverse_link_container_name]\"\n",
    "dataverse_customer_table_name = '[dataverse_customer_table_name]'\n",
    "\n",
    "dataverse_customer_model_df = spark.read.text(f\"abfss://{dataverse_link_container_name}@{storage_account_name}.dfs.core.windows.net/Microsoft.Athena.TrickleFeedService/{dataverse_customer_table_name}-model.json\")\n",
    "dataverse_customer_model_json = dataverse_customer_model_df.first()[0]\n",
    "dataverse_customer_model = json.loads(dataverse_customer_model_json)\n",
    "\n",
    "attributes = dataverse_customer_model['entities'][0]['attributes']\n",
    "dataverse_customer_table_header = [attribute['name'] for attribute in attributes]\n",
    "\n",
    "# print(dataverse_customer_table_header)\n",
    "\n",
    "schema = StructType(\n",
    "    [StructField(f, StringType(), True) for f in dataverse_customer_table_header]\n",
    ")\n",
    "\n",
    "dataverse_customer_table_df = spark.read.option(\"header\", \"false\").schema(schema).option(\"multiLine\", \"true\").csv(f\"abfss://{dataverse_link_container_name}@{storage_account_name}.dfs.core.windows.net/{dataverse_customer_table_name}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "674c3b2c-802d-4df1-92c2-a9ff7666d5b1",
     "showTitle": true,
     "title": "Save to bronze layer with partition"
    }
   },
   "outputs": [],
   "source": [
    "# Save to bronze layer with partition\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "bronze_container_name = 'bronze'\n",
    "\n",
    "partition_date = datetime.now().strftime(\"%Y-%m-%d\")    #partition\n",
    "\n",
    "dataverse_customer_table_df.write.format(\"csv\").option(\"header\",True) .mode(\"overwrite\").save(f\"abfss://{bronze_container_name}@{storage_account_name}.dfs.core.windows.net/{dataverse_customer_table_name}/{partition_date}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "prepare_bronze",
   "notebookOrigID": 3689329351383206,
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
