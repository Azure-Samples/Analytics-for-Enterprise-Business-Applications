{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Prepare BRONZE\n",
        "\n",
        "The export done by the *Synaps Link for Dataverse* is stored in the *dataverse-ENVIRONMENT-GUID* container. This notebook copies this CSV export to the **BRONZE** layer container.\n",
        "\n",
        "1. Read the model json file from the *dataverse-ENVIRONMENT-GUID* => *Microsoft.Athena.TrickleFeedService* directory and extract the column names. \n",
        "2. Read the csv export data under the table name directory, add the column names to the dataframe.\n",
        "3. Save the dataframe to the Bronze layer. Use date as partition folder name.\n",
        "2. Manually upload the external resident data to the residents_external directiry in the raw container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {},
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-03-23T12:36:42.315092Z",
              "execution_start_time": "2023-03-23T12:36:42.1524472Z",
              "livy_statement_state": "available",
              "parent_msg_id": "b7ef8254-1c16-4582-976d-42d86b4293bf",
              "queued_time": "2023-03-23T12:36:42.005792Z",
              "session_id": "0",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkbasic",
              "state": "finished",
              "statement_id": 6
            },
            "text/plain": [
              "StatementMeta(sparkbasic, 0, 6, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Spark Config\n",
        "\n",
        "spark.conf.set(\"fs.azure.account.auth.type\", \"SAS\")\n",
        "spark.conf.set(\"fs.azure.sas.token.provider.type\", \"com.microsoft.azure.synapse.tokenlibrary.ConfBasedSASProvider\")\n",
        "spark.conf.set(\"spark.storage.synapse.sas\", \"\")\n",
        "\n",
        "storage_account_name = \"stgd365analytics\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-03-23T12:38:54.6151983Z",
              "execution_start_time": "2023-03-23T12:38:52.7228247Z",
              "livy_statement_state": "available",
              "parent_msg_id": "2cf197d4-a817-42b1-9ac9-f8253fd8c11c",
              "queued_time": "2023-03-23T12:38:52.560479Z",
              "session_id": "0",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkbasic",
              "state": "finished",
              "statement_id": 10
            },
            "text/plain": [
              "StatementMeta(sparkbasic, 0, 10, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Dataframe from dataverse table with header\n",
        "\n",
        "import json\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "dataverse_link_container_name = \"dataverse-salestrial-unq48221acf3fc4ed11a10b6045bd016\"\n",
        "dataverse_customer_table_name = 'crc33_customer'\n",
        "\n",
        "dataverse_customer_model_df = spark.read.text(f\"abfss://{dataverse_link_container_name}@{storage_account_name}.dfs.core.windows.net/Microsoft.Athena.TrickleFeedService/{dataverse_customer_table_name}-model.json\")\n",
        "dataverse_customer_model_json = dataverse_customer_model_df.first()[0]\n",
        "dataverse_customer_model = json.loads(dataverse_customer_model_json)\n",
        "\n",
        "attributes = dataverse_customer_model['entities'][0]['attributes']\n",
        "dataverse_customer_table_header = [attribute['name'] for attribute in attributes]\n",
        "\n",
        "# print(dataverse_customer_table_header)\n",
        "\n",
        "schema = StructType(\n",
        "    [StructField(f, StringType(), True) for f in dataverse_customer_table_header]\n",
        ")\n",
        "\n",
        "dataverse_customer_table_df = spark.read.option(\"header\", \"false\").schema(schema).option(\"multiLine\", \"true\").csv(f\"abfss://{dataverse_link_container_name}@{storage_account_name}.dfs.core.windows.net/{dataverse_customer_table_name}/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-03-23T12:39:20.6549605Z",
              "execution_start_time": "2023-03-23T12:39:15.2692151Z",
              "livy_statement_state": "available",
              "parent_msg_id": "81364b04-8152-418b-993a-c717d2ca29ca",
              "queued_time": "2023-03-23T12:39:15.1286203Z",
              "session_id": "0",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparkbasic",
              "state": "finished",
              "statement_id": 12
            },
            "text/plain": [
              "StatementMeta(sparkbasic, 0, 12, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Save to bronze layer with partition\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "bronze_container_name = 'bronze'\n",
        "\n",
        "partition_date = datetime.now().strftime(\"%Y-%m-%d\")    #partition\n",
        "\n",
        "dataverse_customer_table_df.write.format(\"csv\").option(\"header\",True) .mode(\"overwrite\").save(f\"abfss://{bronze_container_name}@{storage_account_name}.dfs.core.windows.net/{dataverse_customer_table_name}/{partition_date}\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
