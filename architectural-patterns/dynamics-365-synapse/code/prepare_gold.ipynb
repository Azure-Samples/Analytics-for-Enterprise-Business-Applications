{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Prepare SILVER\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {}
      },
      "source": [
        "# Spark Config\r\n",
        "\r\n",
        "spark.conf.set(\"fs.azure.account.auth.type\", \"SAS\")\r\n",
        "spark.conf.set(\"fs.azure.sas.token.provider.type\", \"com.microsoft.azure.synapse.tokenlibrary.ConfBasedSASProvider\")\r\n",
        "spark.conf.set(\"spark.storage.synapse.sas\", \"?sv=2021-12-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2023-08-29T21:29:15Z&st=2023-03-23T12:29:15Z&spr=https&sig=ghlTFjtWaYvdXVmKQJj8uM5gt4C5P9bhuuRsoo7Y4sk%3D\")\r\n",
        "\r\n",
        "storage_account_name = \"stgd365analytics\"\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql.functions import col\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "dataverse_customer_table_name = 'crc33_customer'\r\n",
        "customer_spend = 'customer_spend'\r\n",
        "silver_container_name = 'silver'\r\n",
        "gold_customer_table_name = 'customer'\r\n",
        "gold_container_name = 'gold'\r\n",
        "partition_date = datetime.now().strftime(\"%Y-%m-%d\")\r\n",
        "\r\n",
        "customer_spend_df = spark.read.parquet(f\"abfss://{silver_container_name}@{storage_account_name}.dfs.core.windows.net/{customer_spend}\")\r\n",
        "dataverse_customer_df = spark.read.parquet(f\"abfss://{silver_container_name}@{storage_account_name}.dfs.core.windows.net/{dataverse_customer_table_name}\")\r\n",
        "\r\n",
        "customer_df = dataverse_customer_df.join(customer_spend_df,dataverse_customer_df.cid ==  customer_spend_df.cid,\"inner\").drop(customer_spend_df.cid)\r\n",
        "\r\n",
        "customer_df.write.mode('append').parquet(f\"abfss://{gold_container_name}@{storage_account_name}.dfs.core.windows.net/{gold_customer_table_name}\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}